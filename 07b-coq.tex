\section{\coq Mechanization}
\label{sec:coq}

Our \coq mechanization 
proves both type safety and
denotation soundness, \ie all the statements of~\S~\ref{sec:soundness}
and serves as a comparison for the metatheoretical 
development abilities of the two theorem provers. 
%is a translation from \lh
%and was built to compare the two developments. 
%
%All theorems from~\S~\ref{sec:soundness} are proven in \coq. %(\ie the proof has no \ha{Admitted}.)% and zero trusted code base. 
%
In \coq, 
Req. \ref{lem:prim-typing} 
is proved (using \coq's interactive development)
and type denotations (of \Cref{fig:den})
are defined as recursive functions using 
Equations~\cite{10.1145/3341690}, 
which make both the 
definition the denotational implementation
of the implication (\S~\ref{sec:typing:implication:denotational})
and the proof  
the denotational soundness (\Cref{lem:denote-sound-first})
possible. 
\begin{fullversion}
    The implication judgment
    is  axiomatized per Requirement \ref{lem:implication}.
\end{fullversion}
%
To fairly compare the two developments
%In order to better understand the relative strengths 
%and tradeoffs of \lh vs. \coq 
in terms of effort and ergonomics,
we did not use external \coq libraries 
%and implemented our own infrastructure 
because no such libraries exist yet for \lh.
%
\citet{Vazou17} previously compared \lh and \coq 
as theorem provers, but their mechanizations were an order of magnitude
smaller than ours and did not use data propositions (\S~\ref{sec:data-props}),
which permit constructive \lh proofs. 

\mypara{\coq \vs \lh}
\coq has a tiny trusted code base (TCB) 
and strong foundational mechanized soundness 
guarantees \cite{coqcoqcorrect}.
%
In contrast, \lh trusts the Haskell compiler (GHC), 
the SMT solver (Z3), and its constraint generation rules 
which have not been formalized. 
%
This work, \sysrf, serves precisely that purpose: by
formalizing and mechanizing a significant subset of \lh, 
leaving out literals, casts, and data types. 
%
As far as the user experience is concerned, 
\coq metatheoretical developments 
are much faster to check, which was 
expected since \lh comes with expensive 
inference, and can be aided by relevant libraries. 
The two tools come with different kinds of automation: 
tactics \vs SMT, which we found to be useful in \emph{complementary} 
parts of the proofs, pointing the way to possible improvements
for both verification styles.
Finally, \lh facilitates reasoning over 
mutually defined and partial functions. 
%
%Next, we expand upon the last two points
%with snippets from our mechanizations. 
%
\begin{fullversion}
We begin by looking at aspects of mechanized metatheory in \coq
that are easier or more feasible than in \lh, and then we
turn to aspects that are easier in \lh.
\end{fullversion}

\mypara{Negative Occurrences and \coq's Equations}\NV{NEW CHECK}
Our original \lh mechanization defined  
denotations as refined data propositions
and proved denotational soundness. 
Though, we realized that the definition 
of the function type denotation
has a negative occurrence and permitting negative occurrences
can, in general, lead to unsoundness~\cite{CP90}. 
%
Our mechanization is the first big-scale user of 
\lh's data propositions thus it was not surprising that it revealed 
this potential unsoundness.
%
To remove this source of unsoundness in \lh, 
we implemented a \coq-style positivity checker
that unsurprisingly rejected the type denotation definitions. 
%
A similar challenge appears in the proof of strong normalization 
of the simply-type lambda calculus that because of negative occurrences
cannot use inductive propositions \cite{Pierce:SF2}.
There, the solution is to use a recursive function \ha{expr -> type -> Prop}
because a definition doesn't need to be computable.
%
In our \coq mechanization, we followed a similar solution, 
but since our definition was not structurally recursive and 
was needed for the proofs, we used 
the full power of \coq's Equations~\cite{10.1145/3341690} to define 
the type denotations. 
%
Unfortunately, a similar approach cannot currently carry over to \lh
because all Haskell functions must be computable and all
\lh annotations must be decidable. Therefore, quantifiers
are neither allowed on the right-hand side of Haskell
definitions nor in the refinements. % of Liquid types.


\mypara{Tactics and Automation}
\coq's tactics and automation often permit shorter
proofs as lemmas and constructors can be used with the
\ha{apply} tactic without writing out all arguments. 
%
For example, in \lh 
safety (thm.~\ref{lem:soundness})
is encoded using 
Haskell's \ha{Either}
for disjunction
and dependent pairs for existentials.
%
(\ha{Steps} is defined, using data propositions, as the 
% reflexive and transitive 
closure of \ha{Step}.)
%
\begin{mcode}
  safety :: e$_0$:Expr -> t:Type -> e:Expr -> HasTy Empty e$_0$ t 
         -> Steps e$_0$ e -> Either {isVal e}  (e$_i$::Expr, Step e e$_i$)
  safety _e$_0$ t _e e$_0$_has_t e$_0$_evals_e = case e$_0$_evals_e of
     Refl e$_0$ -> progress e$_0$ t e$_0$_has_t       -- $\cmt{e_0 = e}$
     AddStep e$_0$ e$_1$ e$_0$_step_e$_1$ e e$_1$_eval_e ->  -- $\cmt{e_0 \step \evalsTo{e_1}{e}}$
       safety e$_1$ t e (preservation e$_0$ t e$_0$_has_t e$_1$ e$_0$_step_e$_1$) e$_1$_eval_e
\end{mcode}
%
%\begin{fullversion}
    The reflexive case is proved by \ha{progress}.
    In the inductive case the evaluation
    sequence is $e_0 \step \evalsTo{e_1}{e}$
    and the proof goes by induction,
    using preservation to ensure that
    $e_1$ is typed.  
%\end{fullversion}
%
In \coq safety is proved 
without any of the three fully applied calls above:
%
\begin{mcode}
  Theorem safety : forall (e$_0$ e:expr) (t:type),
     Steps e$_0$ e -> HasTy Empty e$_0$ t -> isVal e \/ exists e$_i$, Steps e e$_i$.
  Proof. intros; induction H.
    - (* Refl *) apply progress with t; assumption.
    - (* Add  *) apply IHSteps; apply preservation with e; assumption. Qed.
\end{mcode}
%
Automation tactics could make this proof even shorter,
but we retain the essential proof structure.

\mypara{Mutual Recursion}
\lh makes it easy to define and work with mutually
recursive data types, 
such as our typing and subtyping judgments,
and to prove mutually inductive lemmas.
%
\begin{fullversion}
    Similarly, our 
    expressions, types, and predicates are
    three mutually recursive data types.
\end{fullversion}
%
Mutually recursive types are not a natural fit for \coq: 
the automatically generated induction principles do not work,
so we need to use the \ha{Scheme} keyword to generate  
suitable principles.
%
Theorems involving these types cannot be
broken up into separate lemmas for each type 
involved. 
%Rather, a unified approach must
%be taken which leads to theorems that are difficult to use
%directly in the \ha{rewrite} tactic.
%
Rather, one combined statement must be given,
which is difficult to use %directly 
in the \ha{rewrite} tactic.


Another weakness of \coq is that all 
information about the hypothesis is lost during the induction
tactic, so the normal structural \ha{induction} tactic only works when
a judgment contains no information, \ie the data constructor
is instantiated solely with universally quantified
variables. 
%
For instance, in the proof of the weakening~\Cref{lem:weakening},
to do structural induction on %a judgment like 
\ha{HasTy (concat g g')  e t}
we must introduce a universally quantified variable \ha{g0}
and strengthen the theorem with the hypothesis
\ha{g0 = concat g g'}.
%
While the standard library contains an ``experimental'' tactic 
\ha{dependent induction}, we also need to work with 
the special mutual induction principles that we generate for
our types, so we have to directly instantiate the principle
with a strengthened, complex hypothesis%. 
%\begin{comment}
and state the lemma as:
\begin{mcode}
  Lemma lem_weaken_typ' : ( forall (g0 : env) (e : expr) (t : type),
    HasTy g0 e t ->( forall (g g' : env) (x : vname) (t_x : type),
        g0 = concatE g g' ->unique g ->unique g' ->
        (binds g) $\cap $ (binds g') = empty ->~ (in_env x g) ->~ (in_env x g')
        ->HasTy (concatE (Cons x t_x g) g') e t ) ) /\ (
  forall (g0 : env) (t : type) (t' : type),
    Subtype g0 t t' ->( forall (g g' : env) (x : vname) (t_x : type),
        g0 = concatE g g' ->unique g ->unique g' -> 
        (binds g) $\cap $ (binds g') = empty ->~ (in_env x g) ->~ (in_env x g')
        ->Subtype (concatE (Cons x t_x g) g') t t') ).
\end{mcode}
%Proof. apply ( judgments_mutind 
%  (fun (g0 : env) (e : expr) (t : type) (p_e_t : Hastype g0 e t) => 
%    forall (g g':env) (x:vname) (t_x:type),
%      g0 = concatE g g' -> unique g -> unique g'
%                         -> intersect (binds g) (binds g') = empty
%                         -> ~ (in_env x g) -> ~ (in_env x g') 
%                         -> Hastype (concatE (Cons x t_x g) g') e t )
%  (fun (g0 : env) (t : type) (t' : type) (p_t_t' : Subtype g0 t t') => 
%    forall (g g':env) (x:vname) (t_x:type),
%      g0 = concatE g g' -> unique g -> unique g'
%                         -> intersect (binds g) (binds g') = empty
%                         -> ~ (in_env x g) -> ~ (in_env x g') 
%                         -> Subtype (concatE (Cons x t_x g) g') t t')  
%  ); ...
%
%\end{comment}
By contrast, in \lh we can state two separate mutually recursive
lemma functions for weakening: one for typing
and one for subtyping.
%
Then we may call either lemma in their own proofs on any
smaller instance of the typing (resp. subtyping) judgment.
%
%\begin{fullversion}
In practice, developments in \coq sidestep some of these issues
by collapsing the language of terms, types, \etc 
into a single inductive data type. 
%
This approach has the advantage of reducing the number of substitution
operations, but allows highly ungrammatical
combinations like \ha{App Bool False} into our syntax. 
%
We could still use this approach 
combined with a pre-term encoding common in \coq developments,  
but we preferred to keep a closer comparison to the \lh mechanization.
%The solution to this is to define these expressions to be pre-terms and then
%to define terms and types to be an inductive proposition consisting of a 
%pre-term and a proof of being a well-formed term or type. 
% 
%We avoided this common approach in order to keep a closer comparison
%to the \lh mechanization.
%\end{fullversion}

%Sometimes, a lemma cannot be proven 
%For example, the Transitivity of Subtyping cannot be proven 
%by structural induction on the two subtyping judgments because
%in one case we need to use the Substitution Lemma before
%invoking the inductive hypothesis.
%Instead, it must proceed by induction on the combined size of
%the three types involved. In \coq, this requires defining and 
%arguing about an ad-hoc measure and encoding strong induction
%over \ha{Nat} into the statement of the Transitivity Lemma itself.

\mypara{Partial Functions} 
\lh facilitates the definition of partial Haskell functions 
and proves totality with respect to the refined types,
usually automatically, without 
having to reason about impossible cases in mechanized proofs.
%
For instance, our syntax does not contain an explicit 
\ha{error} value, so we only want the function
$\delta(c,\sval)$ to be defined where $\app{c}{\sval}$ can 
step in our semantics.
%
This is straightforward in \lh: we define a predicate 
\ha{isCompat :: Prim ->Value ->Bool} and refine the input
types of $\delta$ to satisfy \ha{isCompat}.
%
%\begin{fullversion}
% NV: isCompat' is not even used in the text jeje
%\begin{mcode}
%  Definition isCompat' (c : prim) (e : expr) : bool :=
%    match c, e with | And , (Bc _)      => true
%                    | Or  , (Bc _)      => true
%                    ...
%                    | _        , _      => false end.
%\end{mcode}
%
%\end{fullversion}
In \coq a more roundabout approach is needed: we have to define 
\ha{isCompat} as an inductive type and include this object as
an explicit argument to our $\delta$ function:
%\begin{fullversion}
%
\begin{mcode}
  Inductive isCompat : prim -> expr -> Set : =   
    | isCpt_And  : forall b, isCompat And (Bc b)
    | isCpt_Or   : forall b, isCompat Or  (Bc b) 
    ...
\end{mcode}
%
%\end{fullversion}
However, this makes it harder to prove the determinism of our
semantics due to the dependence on the proof object. 
%
One solution would be to define a partial version of 
$\delta$ with type
\ha{Prim ->Expr ->option Expr} and prove the two functions
always agree regardless of proof object, \eg using \emph{subset types};
but since  each value
comes wrapped with a term-level proof object,
agreement proofs would require a \emph{Proof Irrelevance} axiom.
%, and 
%as reasoning about equality between objects of a 
%subset type is not possible in general sans 
%a \emph{Proof Irrelevance} axiom \cite{Vazou17}. 
%
